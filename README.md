This is an implementation of a basic GPT, trained on works of William Shakespeare. This model is simpler because it's trained at character level tokens, unlike on larger word like tokens in actual LLMs. This implementation is based on [the video](https://www.youtube.com/watch?v=kCc8FmEb1nY) by Andrej Karpathy. The implementation is based on the famous paper [Attention is all you need](https://arxiv.org/abs/1706.03762). For better understanding the architecture, I used [this excellent video](https://www.youtube.com/watch?v=eMlx5fFNoYc) from 3Blue1Brown youtube channel